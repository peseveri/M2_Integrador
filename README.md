# M2 Integrador - Pipeline ETL con Docker y Airflow

¬°Bienvenido al proyecto **M2 Integrador**!  
Este repositorio contiene la implementaci√≥n de un pipeline **ETL** (Extracci√≥n, Transformaci√≥n y Carga) para un conjunto de datos de **Airbnb en Nueva York**.

El proyecto est√° dise√±ado para ser **robusto**, **escalable** y **reproducible**, utilizando tecnolog√≠as de contenedores como **Docker** y orquestaci√≥n con **Apache Airflow**.

---

## üóÇÔ∏è Estructura del Proyecto

La estructura del repositorio se organiza de la siguiente manera:

### `Avance 1`
Contiene la documentaci√≥n del proyecto, incluyendo un archivo de arquitectura que describe el dise√±o del pipeline y las decisiones t√©cnicas.

### `Avance2_3_4`
Contiene todo el c√≥digo fuente del pipeline ETL:

- `dags/` - Archivos DAG de Airflow para orquestar el flujo de trabajo.
- `data_sources/` - Ubicaci√≥n del archivo de datos CSV original.
- `raw_data/` - Aca van a estar los datos extra√≠dos y procesados en la primera etapa.
- `scripts/` - Scripts de Python para la ingesta y transformaci√≥n.
- `docker-compose.yml` - Archivo principal para levantar el proyecto con Docker y Airflow.
- `docker-compose_local.yml` - Versi√≥n alternativa para un entorno de desarrollo local.
- `Dockerfile.*` - Archivos de Docker para construir las im√°genes de los servicios.
- `test_pipeline.py` - Script para validar que los datos se han procesado correctamente.

---

## üöÄ C√≥mo levantar el proyecto

La forma recomendada de ejecutar el pipeline es a trav√©s de **Airflow**, que permite visualizar y gestionar las tareas.

### ‚úÖ Con Airflow y Docker Compose

1. **Levantar el entorno de Docker**

   Aseg√∫rate de tener **Docker** y **Docker Compose** instalados.  
   Desde la carpeta `Avance2_3_4`, ejecut√°:

   ```bash
   docker-compose -f docker-compose.yml up -d
   ```
2. **Acceder a la interfaz de Airflow**

    Una vez que los contenedores est√©n listos, acced√© a: http://localhost:8080

3. **Ejecutar el DAG**
    
    - Inici√° sesi√≥n con usuario: admin y contrase√±a: admin

    - En la interfaz, busc√° el DAG llamado etl_pipeline_docker y activalo.

    - Dispar√° una nueva ejecuci√≥n (Trigger) del DAG para que se ejecute el pipeline ETL completo.

## üîÑ Flujo de Trabajo de CI/CD

El proyecto utiliza GitHub Actions para automatizar el pipeline de CI/CD.
Este flujo se ejecuta en cada push o pull_request a la rama main y realiza las siguientes tareas:

Build üõ†Ô∏è: Construcci√≥n de las im√°genes Docker.

Test üß™: Ejecuci√≥n de tareas de ingesta y transformaci√≥n dentro de contenedores.

Validation ‚úÖ: Verificaci√≥n de que los datos fueron procesados correctamente y los archivos de salida existen en las capas Silver y Gold.

## C√≥mo usar el Jupyter Notebook para an√°lisis

Una vez que el pipeline ETL ha terminado y los datos se han guardado en la capa Gold, puedes usar el contenedor de Jupyter para realizar an√°lisis y crear dashboards.

Levantar el entorno:
Aseg√∫rate de que los contenedores de Docker est√©n corriendo con el comando docker-compose -f docker-compose.yml up -d.

Acceder a Jupyter Notebook:

Abre tu navegador web y navega a http://localhost:8888.

No se requiere contrase√±a, ya que el contenedor est√° configurado para no pedirla.

Ejecutar el an√°lisis:

Dentro del entorno de Jupyter, navega a la carpeta notebooks.

Abre el archivo analisis_gold_data.ipynb.

Ejecuta las celdas del notebook para cargar los datos de gold_data y ver las visualizaciones.

Este notebook te permite interactuar con los datos procesados y sirve como punto de partida para an√°lisis m√°s avanzados o la creaci√≥n de reportes.


