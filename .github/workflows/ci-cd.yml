name: Pipeline ETL CI/CD

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  build-and-test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Paso 1: Levantar todos los servicios con Airflow
      - name: Start Docker Compose with Airflow
        run: docker compose -f Avance2_3_4/docker-compose.yml up --build -d

      # Paso 2: Esperar a que el webserver de Airflow esté saludable
      # Esto asegura que los comandos de Airflow funcionen
      - name: Wait for Airflow Webserver to be ready
        run: |
          echo "Esperando a que Airflow esté listo..."
          sleep 60
          
      # Paso 3: Disparar el DAG desde el contenedor del webserver de Airflow
      # Ejecuta el comando Airflow CLI dentro del contenedor "airflow-webserver"
      - name: Trigger Airflow DAG
        run: |
          docker compose -f Avance2_3_4/docker-compose.yml exec -T airflow-webserver airflow dags trigger etl_pipeline_docker
          # Esperar a que el DAG termine. Esto puede variar, 
          # por lo que un tiempo de espera fijo es un buen punto de partida.
          sleep 120
      
      # Paso 4: Instalar las dependencias de Python para el script de prueba
      - name: Set up Python for tests
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas pyarrow

      # Paso 5: Ejecutar las pruebas de validación de datos
      - name: Run data validation tests
        run: python Avance2_3_4/test_pipeline.py

      # Paso 6: Limpiar los contenedores
      - name: Clean up Docker containers
        if: always()
        run: docker compose -f Avance2_3_4/docker-compose.yml down
